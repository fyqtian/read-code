### 丁奇45讲

有些时候 MySQL 占用**内存涨得特别快**，这是因为 MySQL 在执行过程中临**时使用的内存是管理在连接对象里面的**。这些资源会在**连接断开的时候才释放**。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。



1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2. 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 **mysql_reset_connection** 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



### 查询缓存

好在 MySQL 也提供了这种“按需使用”的方式。**你可以将参数 query_cache_type 设置成 DEMAND**，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样：

需要注意的是，**MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了**。

在一个表上有更新的时候，**跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空**。这也就是我们一般不建议使用查询缓存的原因。

```
select SQL_CACHE * from T where ID=10；
```



### 日志

WAL 技术，WAL 的全称是 Write-Ahead Logging

### Redo log

redo log **是物理日志**，记录的是“在某个数据页上做了什么修改”

redo log 是循环写的，空间固定会用完

**redo log 是 InnoDB 引擎特有的日志**

当有一条记录**需要更新的时候**，InnoDB 引擎就会先把**记录写到 redo log**里面，**并更新内存**，这个时候更新就算完成了

同时，InnoDB 引擎会在适当的时候，**将这个操作记录更新到磁盘里面**，而这个更新往往是在**系统比较空闲的时候做**。

InnoDB 的 **redo log 是固定大小的**，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，写到末尾就又回到开头循环写

<img src="../images/redo.png" style="zoom:50%;" />

**write pos 是当前记录的位置**，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**。



1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，**同时将这个更新操作记录到 redo log 里面**，此时 redo log 处于 **prepare** 状态。然后告知执行器执行完成了，随时可以提交事务。
4. **执行器生成这个操作的 binlog，并把 binlog 写入磁盘**。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 **redo log 改成提交（commit）状态**，更新完成

### 两阶段提交

为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：**怎样让数据库恢复到半个月内任意一秒的状态？**

由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

1. **先写 redo log 后写 binlog**。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
   但是由于 binlog 没写完就 crash 了，**这时候 binlog 里面就没有记录这个语句**。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。
   然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. **先写 binlog 后写 redo log**。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。



innodb_flush_log_at_trx_commit = 1 表示每次事务的 redo log 都直接持久化到磁盘

sync_binlog = 1 表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，

### binlog

Server 层也有自己的日志，称为 binlog（归档日志）

binlog 是**逻辑日志**，记录的是**这个语句的原始逻辑**，比如“给 ID=2 这一行的 c 字段加 1 ”。

binlog 是可以追加写入的。“追加写”是指 binlog **文件写到一定大小后会切换到下一个**，**并不会覆盖以前的日志**





## 追问 1：MySQL 怎么知道 binlog 是完整的?

1. 如果 **redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交**；
2. 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：
   a. 如果是，则提交事务；
   b. 否则，回滚事务。

回答：一个事务的 binlog 是有完整格式的：

- statement 格式的 binlog，最后会有 COMMIT；
- row 格式的 binlog，最后会有一个 XID event。
- 另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。

## 追问 2：redo log 和 binlog 是怎么关联起来的?

回答：它们有一个共同的数据字段，叫 XID。**崩溃恢复的时候，会按顺序扫描 redo log：**

- 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
- 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。



## 追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?

回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。



## 追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？

回答：其实，两阶段提交是经典的分布式系统问题，并不是 MySQL 独有的。

如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。

对于 InnoDB 引擎来说，**如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）**。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。

两阶段提交就是为了给所有人一个机会，当每个人都说“我 ok”的时候，再一起提交。







### 事物

show variables like 'transaction_isolation';

ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）

脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）

读未提交（read uncommitted）、

读提交（read committed）、

可重复读（repeatable read）

和串行化（serializable ）

- 读未提交是指，一个**事务还没提交**时，它做的**变更就能被别的事务看到**。
- 读提交是指，一个**事务提交之后**，它做的**变更才会被其他事务看到**。
- 可重复读是指，**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的**。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



建议你总是使用 **set autocommit=1,** 通过显式语句的方式来启动事务。



```
查找长事物
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```



在 MySQL 中，实际上**每条记录在更新的时候都会同时记录一条回滚操作**。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。



什么时候删除日志，就是当系统里没有比这个回滚日志更早的 read-view 的时候。

<img src="../images/rollback.png" alt="rollback" style="zoom:50%;" />





### 锁



**另一类表级的锁是 MDL（metadata lock)。**MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。



因此，在 MySQL 5.5 版本中引入了 MDL，**当对一个表做增删改查操作的时候，加 MDL 读锁**；**当要对表做结构变更操作的时候，加 MDL 写锁。**

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行
- MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。







### **行锁**

在 InnoDB 事务中，**行锁是在需要的时候才加上的**，但并不是不需要了就立刻释放，**而是要等到事务结束时才释放**。这个就是两阶段锁协议。

**start transaction with consistent**



mvcc 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。

在可重复读隔离级别下（**rr**），事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。

InnoDB 里面每个事务有一个唯一的事务 ID，**叫作 transaction id。**它是在事务开始的时候向 InnoDB 的事务系统申请的，是按**申请顺序严格递增的**。

而每行数据也都是有**多个版本的**。每次事务更新数据的时候，都会生成一个新的数据版本，**并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。**同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。







**change buffer** 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 

唯一索引更新不能使用

第二种情况是，**这个记录要更新的目标页不在内存中**。这时，InnoDB 的处理流程如下：

- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

如果所有的**更新后面**，都马上**伴随着对这个记录的查询**，**那么你应该关闭 change buffer**。而在其他情况下，change buffer 都能提升更新性能。

普通索引和唯一索引应该怎么选择。其实，**这两类索引在查询能力上是没差别的**，主要考虑的是对更新性能的影响



**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**



analyze table t 

其实，如果只是索引统计不准确，通过 analyze 命令可以解决很多问题，但是前面我们说了，优化器可不止是看扫描行数。







### 幻读

**幻读只会放声在当前读**

**在可重复读隔离级别下**，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现

也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。





**我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。**

1. 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是**前开后闭区间**。
2. 原则 2：查找过程中访问到的对象才会加锁。
3. 优化 1：索引上的等值查询，**给唯一索引加锁的时候，next-key lock 退化为行锁。**
4. 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
5. 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
